These scripts are used for exporting data from Mongo to add to postgres.  They work as follows.

1. Start sage from within the lmfdb folder
2. Attach automate_export.py (from this folder)
3. Run `rough("export.py", "import_file)`, where exprt_file and import_file are filenames for the autogenerated export and import scripts
4. Attach the autogenerated export script.  This will make a function `export_all` available.
5. Call `export_all()`.  This will create text files to be import into postgres.  Note that the location of the resulting files is by default inside an `exports` folder (which will further be inside your lmfdb folder if you don't change directories).  You should create this exports folder before running the script for the first time.  If there are failures in exporting a particular table, it will print the error message and move in.  You can rerun the specific export functions (e.g. `export_ec_nfcurves()`) that failed until it is successful.
6. Attach export_special.py (from this folder)
7. Call `export_special()`.  This will create text files in the same folder as above, for the collections that needed special handling, as well as an import_special.py file.
8. scp the text files into `/scratch/importing/` on the postgres server (if you want to use a different folder, you should edit these scripts).  Also scp import.py and import_special.py to the postgres server.
9. On the postgres server, start a copy of sage inside the lmfdb folder.
10. Attach import.py and import_special.py
11. Call `import_all()` and `import_special()`.  If there are errors in the copy stage, you will need to drop the table by, e.g., `db.drop_table("ec_nfcurves")` before calling `import_ec_nfcurves()` again.
